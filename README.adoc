= Exploring Spring Batch Framework
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2023-12
// Metadata
:description: This repository contains an example of how to run Batch processes on OpenShift using Argo Workflows.
:keywords: openshift, red hat, Batch, Argo, workflows, Spring
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]
// End: Enable admonition icons


This repository contains an example of how to run Batch processes on OpenShift using Argo Workflows.

// Create the Table of contents here
toc::[]

== Introduction


This demo contains all you need to deploy Argo Workflows to coordinate the deployment and execution of batch processes created using Spring Batch. For that reason, this repository contains the following sections:

* First, a quick mechanism to deploy **ArgoCD**. https://argoproj.github.io/cd[Argo CD] automates the deployment of the desired application states in the specified target environments. We will use it to deploy Argo Workflows and our Spring Batch application.
* Second, **Argo Workflows**. https://argoproj.github.io/workflows[Argo Workflows] is a container-native workflow engine for orchestrating parallel jobs on Kubernetes. We will use it to handle the execution of the batch processes.
* Third, **Spring Batch**. https://spring.io/projects/spring-batch[Spring Batch] is a processing framework designed for robust execution of jobs. We will use it to load data from a CSV file, process it, and store it in a Database deployed on OpenShift.

NOTE: All of this will be deployed and automated on OpenShift using the GitOps principles. If you don't have an OpenShift cluster, https://github.com/alvarolop/ocp-installation[check this repo] on how to deploy an OCP cluster on-premise.

TIP: `Data Processing` is one of the https://argoproj.github.io/argo-workflows/use-cases/data-processing/[typical use cases] for Argo Workflows.


== ArgoCD 

There are many ways to install an ArgoCD instance. Among all of them, I've chosen to use https://docs.openshift.com/gitops/1.11/understanding_openshift_gitops/about-redhat-openshift-gitops.html[Red Hat's OpenShift GitOps]. Red Hat OpenShift GitOps is based on the open source project https://argo-cd.readthedocs.io/en/stable/[Argo CD] and provides a similar set of features to what the upstream offers, with additional automation, integration into Red Hat OpenShift Container Platform and the benefits of Red Hat's enterprise support, quality assurance and focus on enterprise security.


The installation of OpenShift GitOps is based on an operator that is installed using the Operator Lifecycle Manager (OLM). To simplify the installation of the operator and the creation of the ArgoCD instance, I've created a simple script that you can run on a clean OCP cluster:

[source, bash]
----
./10-argocd/auto-install.sh
----




== Argo Workflows


There are, also, many official ways to deploy Argo Workflows, all of them on https://argoproj.github.io/argo-workflows/installation/[this page of the documentation]. Apart from the quick-start installation, there are two mechanisms, the single-YAML release manifests and the community-maintained Helm Charts. For this exercise, we are going to use the Helm Charts, as it provides more flexibility and adaptability. Also, we are going to use an ArgoCD application to deploy the chart on OpenShift.


[source, bash]
----
./20-argo-workflows/auto-install.sh
----

From the basic Helm Chart provided in their https://github.com/argoproj/argo-helm/tree/main/charts/argo-workflows[GitHub repository], I've also added both a Namespace and a Route to make sure that the resources are not deployed in the `default` namespace.

After that, I have created a modified `values.yaml` to customize the Argo Workflows behavior:

* Provide HA configuration, based on the https://argoproj.github.io/argo-workflows/high-availability/[documentation] and the https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/ci/ha-values.yaml[Helm Chart example values].
* Expose metrics and collect them using a ServiceMonitor according to the https://argoproj.github.io/argo-workflows/metrics/[documentation]. The Helm chart already takes care of that in these https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/ci/enable-metrics-values.yaml[example values].


NOTE: For general info, I recommend this guide about https://blog.argoproj.io/practical-argo-workflows-hardening-dd8429acc1ce[Argo Workflows Hardening].





== Spring Batch application

The Spring Batch application dubbed *Capitalator* is just a demo application from the https://spring.io/guides/gs/batch-processing/[official guides], which has been adapted to the needs of this demo use case:

* New name of the application.
* Exposing metrics using `micrometer` (Still WIP).
* Option to connect to an external PostgreSQL deployed in another OCP namespace, instead of in-memory HyperSQL DB) (Still WIP).

NOTE: If you are not interested in manual push, just continue to the next section.


=== Generate container image locally

A simple Dockerfile is stored in `src/main/docker/Dockerfile.springboot-jar`, and you can manually generate and push the container image to Quay with the following manual steps:


[source, bash]
----
# Generate the Jar file with all the dependencies
mvn clean package

# Add the executable to a container image
podman build -f src/main/docker/Dockerfile.springboot-jar -t spring-boot/spring-batch-capitalator .

# Launch the application
podman run -i --rm spring-boot/spring-batch-capitalator
----


=== Push image to Quay

Then, push the image to Quay using the following commands (Previously login to Quay with an authorized account):

[source, bash]
----
# Tag the image to point to your Quay URL
podman tag spring-boot/spring-batch-capitalator quay.io/alopezme/spring-batch-capitalator

# Push image to Quay
podman push quay.io/alopezme/spring-batch-capitalator
----

=== Extra: GitHub Actions

As we don't want to manually execute commands to generate and push a container image, I have automated the Build and Push process with a GH Workflow that is triggered every time a new commit is pushed and affects one of the files of the application itself.

Also, if a new git tag in `semver` format is pushed, it will generate an extra image using that tag as a container tag to the Quay repo.

For this to work, it is necessary to create a Robot Account in Quay with write permissions and create the following two secrets in the Git repository:

* *QUAY_REPO_TOKEN*.
* *QUAY_REPO_USERNAME*.


=== Deploy to OpenShift without Argo Workflows

If you don't need any Batch Processing Orchestration, 

[source, bash]
----
# ArgoCD application
oc apply -f argocd-apps/application-capitalator.yaml

# Apply resources directly
oc apply -f 30-postgresql/
----






== PostgreSQL Database

To make the Spring Batch Capitalator example more similar to a real use case, this repository also provides a simple mechanism to deploy a `postgresql` database on a side namespace, so that Capitalator can connect and store the uppercase version of the names. You can deploy the DB either creating an ArgoCD application or applying the resources directly:

[source, bash]
----
# ArgoCD application
oc apply -f argocd-apps/application-postgresql.yaml

# Apply resources directly
oc apply -f 31-postgresql/
----









== Useful resources



